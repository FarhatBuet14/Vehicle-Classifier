####################### Defining the model ##############################

model = Sequential()
model.add(Convolution2D(32, 3, 3 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(Convolution2D(32, 3, 3 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))


model.add(Convolution2D(64, 2, 2 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(Convolution2D(32, 2, 2 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))


model.add(Convolution2D(128, 2, 2 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(Convolution2D(128, 2, 2 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))


model.add(Flatten())
model.add(Dense(128, activation= 'relu' ))
model.add(Dense(num_classes, activation= 'softmax' ))

#################### Summary of the Model ###############################

model.summary()

#################### Compiling the Model ################################
optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(loss= 'categorical_crossentropy' , 
              optimizer= optimizer , metrics=[ 'accuracy' ])

#################### Defining the Checkpoints ###########################
l_r = ReduceLROnPlateau(monitor='val_acc', factor=0.5, 
                                  patience=3, verbose=1, 
                                  min_lr=0.000001)

wigth  = ModelCheckpoint(weightFile, monitor = 'val_categorical_accuracy' )
callbacks = [wigth, l_r]



datagen = ImageDataGenerator(featurewise_center=True,
                            rotation_range=20,
                            width_shift_range=0.2,
                            height_shift_range=0.2,
                            horizontal_flip=True)

datagen.fit(X_train)

model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32),
                    validation_data = datagen.flow(X_val, y_val, batch_size = 32),
                    steps_per_epoch = len(X_train) / 32, 
                    validation_steps = len(X_val) / 16, epochs = epochs, 
                    callbacks = callbacks, verbose = verbose)





Epoch 1/65
430/429 [==============================] - 51s 119ms/step - loss: 0.9019 - acc: 0.5958 - val_loss: 1.2338 - val_acc: 0.6181
Epoch 2/65
430/429 [==============================] - 49s 113ms/step - loss: 0.7372 - acc: 0.6710 - val_loss: 1.6122 - val_acc: 0.5592
Epoch 3/65
430/429 [==============================] - 49s 113ms/step - loss: 0.7009 - acc: 0.6897 - val_loss: 1.7380 - val_acc: 0.6351
Epoch 4/65
430/429 [==============================] - 48s 113ms/step - loss: 0.6610 - acc: 0.7061 - val_loss: 1.6912 - val_acc: 0.6587
Epoch 5/65
430/429 [==============================] - 48s 112ms/step - loss: 0.6400 - acc: 0.7235 - val_loss: 1.5626 - val_acc: 0.6135
Epoch 6/65
430/429 [==============================] - 48s 113ms/step - loss: 0.6070 - acc: 0.7423 - val_loss: 2.3151 - val_acc: 0.6135
Epoch 7/65
430/429 [==============================] - 48s 112ms/step - loss: 0.6000 - acc: 0.7503 - val_loss: 1.9468 - val_acc: 0.6325

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 8/65
430/429 [==============================] - 48s 111ms/step - loss: 0.5598 - acc: 0.7651 - val_loss: 1.8882 - val_acc: 0.6574
Epoch 9/65
430/429 [==============================] - 48s 112ms/step - loss: 0.5514 - acc: 0.7718 - val_loss: 1.3011 - val_acc: 0.6976
Epoch 10/65
430/429 [==============================] - 48s 113ms/step - loss: 0.5345 - acc: 0.7836 - val_loss: 2.2484 - val_acc: 0.6165
Epoch 11/65
430/429 [==============================] - 48s 113ms/step - loss: 0.5348 - acc: 0.7769 - val_loss: 1.0526 - val_acc: 0.7143
Epoch 12/65
430/429 [==============================] - 49s 114ms/step - loss: 0.5249 - acc: 0.7837 - val_loss: 0.8894 - val_acc: 0.7497
Epoch 13/65
430/429 [==============================] - 47s 109ms/step - loss: 0.5119 - acc: 0.7841 - val_loss: 0.7866 - val_acc: 0.7549
Epoch 14/65
430/429 [==============================] - 47s 109ms/step - loss: 0.5234 - acc: 0.7844 - val_loss: 1.3925 - val_acc: 0.6734
Epoch 15/65
430/429 [==============================] - 47s 109ms/step - loss: 0.5131 - acc: 0.7916 - val_loss: 1.0670 - val_acc: 0.7248
Epoch 16/65
430/429 [==============================] - 48s 110ms/step - loss: 0.5095 - acc: 0.7912 - val_loss: 1.1379 - val_acc: 0.6780

Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 17/65
430/429 [==============================] - 48s 111ms/step - loss: 0.4861 - acc: 0.8036 - val_loss: 1.1999 - val_acc: 0.6888
Epoch 18/65
430/429 [==============================] - 50s 117ms/step - loss: 0.4836 - acc: 0.8028 - val_loss: 1.1034 - val_acc: 0.6980
Epoch 19/65
430/429 [==============================] - 50s 117ms/step - loss: 0.4816 - acc: 0.8077 - val_loss: 0.9341 - val_acc: 0.7402

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 20/65
430/429 [==============================] - 50s 117ms/step - loss: 0.4700 - acc: 0.8091 - val_loss: 0.8115 - val_acc: 0.7359
Epoch 21/65
430/429 [==============================] - 53s 124ms/step - loss: 0.4706 - acc: 0.8083 - val_loss: 0.9505 - val_acc: 0.7084
Epoch 22/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4562 - acc: 0.8147 - val_loss: 0.9692 - val_acc: 0.7091

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 23/65
430/429 [==============================] - 51s 118ms/step - loss: 0.4554 - acc: 0.8148 - val_loss: 0.8933 - val_acc: 0.7170
Epoch 24/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4594 - acc: 0.8161 - val_loss: 0.8125 - val_acc: 0.7245
Epoch 25/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4470 - acc: 0.8164 - val_loss: 0.8310 - val_acc: 0.7408

Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
Epoch 26/65
430/429 [==============================] - 51s 119ms/step - loss: 0.4553 - acc: 0.8145 - val_loss: 0.9133 - val_acc: 0.7222
Epoch 27/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4502 - acc: 0.8213 - val_loss: 0.8726 - val_acc: 0.7320
Epoch 28/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4464 - acc: 0.8198 - val_loss: 0.8404 - val_acc: 0.7327

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.
Epoch 29/65
430/429 [==============================] - 52s 122ms/step - loss: 0.4486 - acc: 0.8141 - val_loss: 0.8510 - val_acc: 0.7271
Epoch 30/65
430/429 [==============================] - 50s 117ms/step - loss: 0.4558 - acc: 0.8150 - val_loss: 0.8582 - val_acc: 0.7176
Epoch 31/65
430/429 [==============================] - 53s 122ms/step - loss: 0.4504 - acc: 0.8165 - val_loss: 0.8679 - val_acc: 0.7212

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-06.
Epoch 32/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4549 - acc: 0.8170 - val_loss: 0.8573 - val_acc: 0.7219
Epoch 33/65
430/429 [==============================] - 51s 118ms/step - loss: 0.4474 - acc: 0.8209 - val_loss: 0.8364 - val_acc: 0.7304
Epoch 34/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4475 - acc: 0.8195 - val_loss: 0.8673 - val_acc: 0.7274
Epoch 35/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4470 - acc: 0.8148 - val_loss: 0.8647 - val_acc: 0.7219
Epoch 36/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4427 - acc: 0.8204 - val_loss: 0.8284 - val_acc: 0.7235
Epoch 37/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4451 - acc: 0.8192 - val_loss: 0.8517 - val_acc: 0.7284
Epoch 38/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4500 - acc: 0.8160 - val_loss: 0.8413 - val_acc: 0.7330
Epoch 39/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4497 - acc: 0.8188 - val_loss: 0.8802 - val_acc: 0.7268
Epoch 40/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4457 - acc: 0.8205 - val_loss: 0.8397 - val_acc: 0.7369
Epoch 41/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4391 - acc: 0.8199 - val_loss: 0.8654 - val_acc: 0.7274
Epoch 42/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4531 - acc: 0.8111 - val_loss: 0.8587 - val_acc: 0.7271
Epoch 43/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4469 - acc: 0.8173 - val_loss: 0.8281 - val_acc: 0.7297
Epoch 44/65
430/429 [==============================] - 54s 125ms/step - loss: 0.4490 - acc: 0.8164 - val_loss: 0.8766 - val_acc: 0.7268
Epoch 45/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4446 - acc: 0.8184 - val_loss: 0.8602 - val_acc: 0.7291
Epoch 46/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4446 - acc: 0.8187 - val_loss: 0.8458 - val_acc: 0.7294
Epoch 47/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4443 - acc: 0.8175 - val_loss: 0.8479 - val_acc: 0.7307
Epoch 48/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4477 - acc: 0.8188 - val_loss: 0.8474 - val_acc: 0.7271
Epoch 49/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4477 - acc: 0.8192 - val_loss: 0.8209 - val_acc: 0.7327
Epoch 50/65
430/429 [==============================] - 52s 120ms/step - loss: 0.4414 - acc: 0.8200 - val_loss: 0.8534 - val_acc: 0.7251
Epoch 51/65
430/429 [==============================] - 53s 124ms/step - loss: 0.4425 - acc: 0.8208 - val_loss: 0.8335 - val_acc: 0.7313
Epoch 52/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4448 - acc: 0.8231 - val_loss: 0.8518 - val_acc: 0.7297
Epoch 53/65
430/429 [==============================] - 51s 119ms/step - loss: 0.4477 - acc: 0.8152 - val_loss: 0.8449 - val_acc: 0.7261
Epoch 54/65
430/429 [==============================] - 53s 123ms/step - loss: 0.4468 - acc: 0.8207 - val_loss: 0.8683 - val_acc: 0.7264
Epoch 55/65
430/429 [==============================] - 51s 119ms/step - loss: 0.4503 - acc: 0.8193 - val_loss: 0.8325 - val_acc: 0.7343
Epoch 56/65
430/429 [==============================] - 51s 118ms/step - loss: 0.4444 - acc: 0.8210 - val_loss: 0.8150 - val_acc: 0.7313
Epoch 57/65
430/429 [==============================] - 49s 114ms/step - loss: 0.4473 - acc: 0.8185 - val_loss: 0.8197 - val_acc: 0.7271
Epoch 58/65
430/429 [==============================] - 52s 122ms/step - loss: 0.4492 - acc: 0.8154 - val_loss: 0.8391 - val_acc: 0.7255
Epoch 59/65
430/429 [==============================] - 51s 119ms/step - loss: 0.4413 - acc: 0.8235 - val_loss: 0.8435 - val_acc: 0.7170
Epoch 60/65
430/429 [==============================] - 53s 124ms/step - loss: 0.4485 - acc: 0.8141 - val_loss: 0.8209 - val_acc: 0.7287
Epoch 61/65
430/429 [==============================] - 53s 122ms/step - loss: 0.4440 - acc: 0.8214 - val_loss: 0.8272 - val_acc: 0.7421
Epoch 62/65
430/429 [==============================] - 54s 125ms/step - loss: 0.4403 - acc: 0.8219 - val_loss: 0.8453 - val_acc: 0.7212
Epoch 63/65
430/429 [==============================] - 53s 124ms/step - loss: 0.4434 - acc: 0.8205 - val_loss: 0.8411 - val_acc: 0.7395
Epoch 64/65
430/429 [==============================] - 53s 124ms/step - loss: 0.4472 - acc: 0.8214 - val_loss: 0.8437 - val_acc: 0.7291
Epoch 65/65
430/429 [==============================] - 52s 121ms/step - loss: 0.4486 - acc: 0.8172 - val_loss: 0.8125 - val_acc: 0.7291














