# -*- coding: utf-8 -*-
"""
Created on Fri May 11 10:07:19 2018

@author: Suhail
"""

##################### Library Imports ################################

import numpy as np
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense,Flatten,Dropout
from keras.layers.convolutional import Convolution2D,MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot
import os
from keras.optimizers import RMSprop
from sklearn.metrics import confusion_matrix



#################### Environment & Variables ############################

from keras import backend as K
K.set_image_data_format('channels_last')

seed = 7
np.random.seed(seed)
epochs = 65
batch_size = 32
verbose = 1

train_fldr = './Own_Dataset/Train/'
test_fldr = './Own_Dataset/Test/'

weightFile = './Model/best.hdf5'
datafile = './Data/datafile.npz'

####################### Loading the Data ################################


dataset = np.load(datafile)

X_train = dataset['X_train']
y_train = dataset['y_train']
X_test = dataset['X_test']
y_test = dataset['y_test']

X_train = X_train/255
X_test = X_test/255

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

num_classes = y_train.shape[1]
imageSize = X_train.shape[1]


random_seed = 0
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=random_seed)



####################### Defining the model ##############################

model = Sequential()
model.add(Convolution2D(32, 3, 3 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(Convolution2D(32, 3, 3 , 
                        input_shape=(imageSize,imageSize,3),activation= 'relu' ))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))


model.add(Flatten())
model.add(Dense(128, activation= 'relu' ))
model.add(Dense(num_classes, activation= 'softmax' ))

#################### Summary of the Model ###############################

model.summary()

#################### Compiling the Model ################################
optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(loss= 'categorical_crossentropy' , 
              optimizer= optimizer , metrics=[ 'accuracy' ])

#################### Defining the Checkpoints ###########################
l_r = ReduceLROnPlateau(monitor='val_acc', factor=0.5, 
                                  patience=3, verbose=1, 
                                  min_lr=0.000001)

wigth  = ModelCheckpoint(weightFile, monitor = 'val_categorical_accuracy' )
callbacks = [wigth, l_r]



datagen = ImageDataGenerator(featurewise_center=True,
                            rotation_range=20,
                            width_shift_range=0.2,
                            height_shift_range=0.2,
                            horizontal_flip=True)

datagen.fit(X_train)

model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32),
                    validation_data = datagen.flow(X_val, y_val, batch_size = 32),
                    steps_per_epoch = len(X_train) / 32, 
                    validation_steps = len(X_val) / 32, epochs = epochs, 
                    callbacks = callbacks, verbose = verbose)





Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 60, 60, 32)        9248      
_________________________________________________________________
batch_normalization_1 (Batch (None, 60, 60, 32)        128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 30, 30, 32)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 28800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               3686528   
_________________________________________________________________
dense_2 (Dense)              (None, 7)                 903       
=================================================================
Total params: 3,697,703
Trainable params: 3,697,639
Non-trainable params: 64
_________________________________________________________________
Epoch 1/65
141/140 [==============================] - 6s 40ms/step - loss: 1.7016 - acc: 0.3937 - val_loss: 1.4788 - val_acc: 0.4720
Epoch 2/65
141/140 [==============================] - 4s 29ms/step - loss: 1.4415 - acc: 0.4643 - val_loss: 1.4668 - val_acc: 0.4880
Epoch 3/65
141/140 [==============================] - 4s 31ms/step - loss: 1.3575 - acc: 0.4962 - val_loss: 1.3463 - val_acc: 0.5300
Epoch 4/65
141/140 [==============================] - 4s 29ms/step - loss: 1.2886 - acc: 0.5247 - val_loss: 1.2443 - val_acc: 0.5580
Epoch 5/65
141/140 [==============================] - 4s 29ms/step - loss: 1.2622 - acc: 0.5343 - val_loss: 1.2126 - val_acc: 0.5740
Epoch 6/65
141/140 [==============================] - 4s 29ms/step - loss: 1.2059 - acc: 0.5578 - val_loss: 1.2486 - val_acc: 0.5600
Epoch 7/65
141/140 [==============================] - 4s 29ms/step - loss: 1.1757 - acc: 0.5707 - val_loss: 1.2334 - val_acc: 0.5560
Epoch 8/65
141/140 [==============================] - 4s 29ms/step - loss: 1.1388 - acc: 0.5845 - val_loss: 1.1608 - val_acc: 0.5960
Epoch 9/65
141/140 [==============================] - 4s 29ms/step - loss: 1.1164 - acc: 0.5844 - val_loss: 1.1748 - val_acc: 0.5740
Epoch 10/65
141/140 [==============================] - 4s 29ms/step - loss: 1.0849 - acc: 0.6027 - val_loss: 1.1495 - val_acc: 0.6000
Epoch 11/65
141/140 [==============================] - 4s 30ms/step - loss: 1.0889 - acc: 0.5942 - val_loss: 1.1469 - val_acc: 0.6100
Epoch 12/65
141/140 [==============================] - 4s 30ms/step - loss: 1.0642 - acc: 0.6125 - val_loss: 1.1184 - val_acc: 0.5980
Epoch 13/65
141/140 [==============================] - 4s 29ms/step - loss: 1.0416 - acc: 0.6233 - val_loss: 1.0850 - val_acc: 0.6320
Epoch 14/65
141/140 [==============================] - 4s 30ms/step - loss: 1.0284 - acc: 0.6305 - val_loss: 1.1267 - val_acc: 0.5880
Epoch 15/65
141/140 [==============================] - 4s 29ms/step - loss: 1.0076 - acc: 0.6315 - val_loss: 1.1303 - val_acc: 0.5880
Epoch 16/65
141/140 [==============================] - 4s 29ms/step - loss: 1.0007 - acc: 0.6409 - val_loss: 1.0854 - val_acc: 0.6020

Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 17/65
141/140 [==============================] - 4s 30ms/step - loss: 0.9527 - acc: 0.6524 - val_loss: 0.9962 - val_acc: 0.6600
Epoch 18/65
141/140 [==============================] - 4s 29ms/step - loss: 0.9445 - acc: 0.6590 - val_loss: 0.9880 - val_acc: 0.6640
Epoch 19/65
141/140 [==============================] - 4s 29ms/step - loss: 0.9219 - acc: 0.6695 - val_loss: 0.9802 - val_acc: 0.6460
Epoch 20/65
141/140 [==============================] - 4s 29ms/step - loss: 0.9238 - acc: 0.6680 - val_loss: 0.9841 - val_acc: 0.6580
Epoch 21/65
141/140 [==============================] - 4s 29ms/step - loss: 0.9044 - acc: 0.6750 - val_loss: 1.0183 - val_acc: 0.6460

Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 22/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8875 - acc: 0.6752 - val_loss: 1.0051 - val_acc: 0.6620
Epoch 23/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8811 - acc: 0.6788 - val_loss: 0.9987 - val_acc: 0.6480
Epoch 24/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8890 - acc: 0.6782 - val_loss: 0.9830 - val_acc: 0.6620

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 25/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8691 - acc: 0.6881 - val_loss: 0.9746 - val_acc: 0.6640
Epoch 26/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8566 - acc: 0.6952 - val_loss: 0.9652 - val_acc: 0.6600
Epoch 27/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8446 - acc: 0.6996 - val_loss: 0.9265 - val_acc: 0.6720
Epoch 28/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8420 - acc: 0.6928 - val_loss: 1.0095 - val_acc: 0.6560
Epoch 29/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8560 - acc: 0.6940 - val_loss: 0.9006 - val_acc: 0.6880
Epoch 30/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8439 - acc: 0.7042 - val_loss: 0.9668 - val_acc: 0.6620
Epoch 31/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8577 - acc: 0.6879 - val_loss: 0.9447 - val_acc: 0.6820
Epoch 32/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8348 - acc: 0.7022 - val_loss: 0.9747 - val_acc: 0.6780

Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 33/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8611 - acc: 0.6907 - val_loss: 0.9405 - val_acc: 0.6620
Epoch 34/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8454 - acc: 0.6913 - val_loss: 0.9527 - val_acc: 0.6760
Epoch 35/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8379 - acc: 0.7005 - val_loss: 0.9654 - val_acc: 0.6840

Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
Epoch 36/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8396 - acc: 0.7021 - val_loss: 0.9571 - val_acc: 0.6620
Epoch 37/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8323 - acc: 0.7044 - val_loss: 0.9306 - val_acc: 0.7100
Epoch 38/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8308 - acc: 0.7098 - val_loss: 0.9581 - val_acc: 0.6700
Epoch 39/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8302 - acc: 0.7005 - val_loss: 0.9358 - val_acc: 0.6880
Epoch 40/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8311 - acc: 0.7018 - val_loss: 0.9403 - val_acc: 0.6680

Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.
Epoch 41/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8227 - acc: 0.7078 - val_loss: 0.9660 - val_acc: 0.6680
Epoch 42/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8481 - acc: 0.7021 - val_loss: 0.9231 - val_acc: 0.6660
Epoch 43/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8212 - acc: 0.7015 - val_loss: 1.0161 - val_acc: 0.6460

Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-06.
Epoch 44/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8240 - acc: 0.7061 - val_loss: 0.9524 - val_acc: 0.6640
Epoch 45/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8090 - acc: 0.7105 - val_loss: 0.9691 - val_acc: 0.6480
Epoch 46/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8333 - acc: 0.7008 - val_loss: 0.9640 - val_acc: 0.6860
Epoch 47/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8205 - acc: 0.7095 - val_loss: 0.9211 - val_acc: 0.6900
Epoch 48/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8280 - acc: 0.7029 - val_loss: 0.9485 - val_acc: 0.6620
Epoch 49/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8276 - acc: 0.7031 - val_loss: 0.9104 - val_acc: 0.6880
Epoch 50/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8304 - acc: 0.7005 - val_loss: 0.9178 - val_acc: 0.7040
Epoch 51/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8259 - acc: 0.7066 - val_loss: 0.9246 - val_acc: 0.6860
Epoch 52/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8428 - acc: 0.6949 - val_loss: 0.9608 - val_acc: 0.6540
Epoch 53/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8105 - acc: 0.7092 - val_loss: 0.9599 - val_acc: 0.6600
Epoch 54/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8266 - acc: 0.6966 - val_loss: 0.9318 - val_acc: 0.6960
Epoch 55/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8249 - acc: 0.7076 - val_loss: 0.9439 - val_acc: 0.6760
Epoch 56/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8195 - acc: 0.7060 - val_loss: 0.9830 - val_acc: 0.6660
Epoch 57/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8299 - acc: 0.7014 - val_loss: 0.9255 - val_acc: 0.6880
Epoch 58/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8228 - acc: 0.7006 - val_loss: 0.9738 - val_acc: 0.6740
Epoch 59/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8191 - acc: 0.7058 - val_loss: 0.9285 - val_acc: 0.6740
Epoch 60/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8299 - acc: 0.6946 - val_loss: 0.9468 - val_acc: 0.6920
Epoch 61/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8242 - acc: 0.7045 - val_loss: 0.9317 - val_acc: 0.6860
Epoch 62/65
141/140 [==============================] - 4s 30ms/step - loss: 0.8234 - acc: 0.6985 - val_loss: 0.9460 - val_acc: 0.6900
Epoch 63/65
141/140 [==============================] - 4s 29ms/step - loss: 0.8021 - acc: 0.7075 - val_loss: 0.9511 - val_acc: 0.7020
Epoch 64/65
141/140 [==============================] - 4s 30ms/step - loss: 0.8280 - acc: 0.7037 - val_loss: 0.9584 - val_acc: 0.6820
Epoch 65/65
141/140 [==============================] - 4s 30ms/step - loss: 0.8286 - acc: 0.7006 - val_loss: 0.9879 - val_acc: 0.6740